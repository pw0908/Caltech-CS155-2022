\newif\ifshowsolutions
\showsolutionstrue
\input{preamble}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% HEADER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chead{
  {\vbox{
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Set 4\hfill
      January $26^\text{th}$, 2022 \\
    }
  }
}

\begin{document}
\pagestyle{fancy}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% POLICIES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Policies}
\begin{itemize}
  \item \textbf{Due 9 PM, February $2^\text{nd}$}, via Gradescope.
  \item You are free to collaborate on all of the problems, subject to the collaboration policy stated in the syllabus.
  \item In this course, we will be using Google Colab for code submissions. You will need a Google account.
  \item This set uses PyTorch, a Python package for neural networks. We recommend using Google Colab, which comes with PyTorch already installed. There will be a PyTorch recitation to help you get started.
\end{itemize}

\section*{Submission Instructions}
\begin{itemize}
\item Submit your report as a single .pdf file to Gradescope (entry code 7426YK), under "Set 4 Report". 
	\item In the report, \textbf{include any images generated by your code} along with your answers to the questions.
	\item Submit your code by \textbf{sharing a link in your report} to your Google Colab notebook for each problem (see naming instructions below). Make sure to set sharing permissions to at least "Anyone with the link can view". \textbf{Links that can not be run by TAs will not be counted as turned in.} Check your links in an incognito window before submitting to be sure. 
	\item For instructions specifically pertaining to the Gradescope submission process, see \url{https://www.gradescope.com/get_started#student-submission}.
\end{itemize}

\section*{Google Colab Instructions}
For each notebook, you need to save a copy to your drive.
\begin{enumerate}
	\item Open the github preview of the notebook, and click the icon to open the colab preview.
	\item On the colab preview, go to File $\rightarrow$ Save a copy in Drive.
	\item Edit your file name to “lastname_firstname_set_problem”, e.g.”yue_yisong_set4_prob2.ipynb”
\end{enumerate}

\section*{TA Office Hours}
\begin{itemize}
    \item \textbf{Megan Tjandrasuwita [Question 1 and Question 2]}
        \begin{itemize}
            \item Monday 1/31: 3:00 pm - 4:00 pm (Annenberg North Lawn Tent), Tuesday, 2/1: 5:00 pm - 6:00 pm (Zoom)
        \end{itemize}
    \item \textbf{Pantelis Vafidis [Question 3]}
        \begin{itemize}
            \item Monday, 1/31: 6:00 pm - 8:00 pm (Zoom)
        \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROBLEM 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Deep Learning Principles [35 Points]}
\materials{lectures on deep learning}

 For problems A and B, we'll be utilizing the \href{http://playground.tensorflow.org/}{Tensorflow Playground} to visualize/fit a neural network.

\begin{problem}[5]
  Backpropagation and Weight Initialization Part 1
\end{problem}

Fit the neural network at \href{http://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.65409&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}{this link} for about 250 iterations, and then do the same for the neural network at  \href{http://playground.tensorflow.org//\#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.6&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=true&hideText=false}{this link}.  Both networks have the same architecture and use ReLU activations.  The only difference between the two is how the layer weights were initialized -- you can examine the layer weights by hovering over the edges between neurons.

Give a mathematical justification, based on what you know about the backpropagation algorithm and the ReLU function, for the difference in the performance of the two networks.

\begin{subsolution}
 In the back-propagation algorithm, we use:
 \begin{equation}
   \frac{\partial S^l}{\partial X^{l-1}}=W^l
 \end{equation}
 If we initialise our weights to 0, as we did in the second example, there will effectively be no `learning'. Furthermore, after the first layer, due to the ReLU function being $\max{0,S}$, our inputs to subsequent layers will also be zero, as $s^2=w^1x^1$ (as we saw visually in the second example). As such, our weights will never be updated. This is not the case in the first example as we have initialised the weights to be non-zero, allowing `learning' to occur. 
\end{subsolution}

\begin{problem}[5]
  Backpropagation and Weight Initialization Part 2
\end{problem}
Reset the two demos from part i (there is a reset button to the left of the ``Run'' button), change the activation functions of the neurons to sigmoid instead of ReLU, and train each of them for 4000 iterations.

Explain the differences in the models learned, and the speed at which they were learned, from those of part i in terms of the backpropagation algorithm and the sigmoid function.



\begin{subsolution}
 In the case of a sigmoid function, its value for a zero input will be non-zero. This deals with the issue we encountered previously with the ReLU function. Thus, it would seem to imply that we should be able to `teach' a model even if the initial weights are zero. This is indeed the case as even in the case of the second example, we (eventually) see changes in the weights. 

 However, we now encounter the `vanishing gradient' problem; in the case of the first example, due to the weights being initiated at very small values, these get updated very slowly (taking 500 iterations to observe a significant drop in error). This is even worse in the second example where the weights initiated at 0 take 3200 iterations to observe a significant change (even this change is slower than in the first example).

 We also notice that the weights in the second example, even after some training occur, are identical between all the nodes. This is likely because we used the same initial guesses (zero) rather than random initial guesses.
 
 Nevertheless, in contrast to the models in the previous question, we do notice that the boundaries obtained are much smoother when using a sigmoid activation function. This is related to the fact that the sigmoid function is continuous and differentiable.
\end{subsolution}



\problem \textbf{[10 Points]}

When training any model using SGD, it's important to shuffle your data to avoid correlated samples. To illustrate one reason for this that is particularly important for ReLU networks, consider a dataset of 1000 points, 500 of which have positive (+1) labels, and 500 of which have negative (-1) labels. What happens if we train a fully-connected network with ReLU activations using SGD, looping through all the negative examples before any of the positive examples? (Hint: this is called the ``dying ReLU'' problem.)

\begin{solution}
 If we were to train a ReLU network using SGD, only cycling through the 500 data points with negative labels, the weights would develop a heavy negative bias. This can result in a `dead' ReLU where, as the input will be biased towards negative values, due to the nature of ReLU function, the output will always be zero. As we explained previously, if the output from the first layer is zero, the output of all subsequent layers will also be zero, preventing further learning, even when introducing positive examples (in a sense, we will have reached a minima where SGD will be unable to step out of).
\end{solution}



\problem Approximating Functions Part 1 \textbf{[7 Points]}

Draw or describe a fully-connected network with ReLU units that implements the OR function on two 0/1-valued inputs,  $x_1$ and $x_2$.  Your networks should contain the minimum number of hidden units possible.  The OR function $\text{OR}(x_1, x_2)$ is defined as:
\begin{gather*}
\text{OR}(1, 0) \geq 1 \\
\text{OR}(0, 1) \geq 1 \\
\text{OR}(1, 1) \geq 1 \\
\text{OR}(0, 0) = 0
\end{gather*}

Your network need only produce the correct output when $x_1 \in \{0, 1\}$ and $x_2 \in \{0, 1\}$ (as described in the examples above).

\begin{subsolution}
  \begin{figure}[H]
    \begin{center}
    \includegraphics[width=0.5\textwidth]{P1C.pdf}
    \end{center}
    \end{figure}
\end{subsolution}

\problem Approximating Functions Part 2 \textbf{[8 Points]}

What is the minimum number of fully-connected layers (with ReLU units) needed to implement an XOR of two 0/1-valued inputs $x_1, x_2$? Recall that the XOR function is defined as:
\begin{gather*}
\text{XOR}(1, 0) \geq 1 \\
\text{XOR}(0, 1) \geq 1 \\
\text{XOR}(0, 0) = \text{XOR}(1, 1) = 0
\end{gather*}

For the purposes of this problem, we say that a network $f$ computes the XOR function if $f(x_1, x_2) = \text{XOR}(x_1, x_2)$ when $x_1 \in \{0, 1\}$ and $x_2 \in \{0, 1\}$ (as described in the examples above).

Explain why a network with fewer layers than the number you specified cannot compute XOR.


\begin{subsolution}
 Given the XOR($x_1,x_2$) is not linearly separable (the function would look like the classification problem given in the second homework set), we will need at least one hidden layer, as well as an input and output layer. If we only had the latter two, we'd only have a perceptron which will be unable to separate the dataset. 

 Thus we need a minimum of three layers:

 \begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.5\textwidth]{P1D.pdf}
  \end{center}
  \end{figure}
\end{subsolution}


% problem 2
\newpage
\section{Depth vs Width on the MNIST Dataset  [25 Points, 6 EC Points]}

\textit{Relevant Materials: Lectures on Deep Learning}

MNIST is a classic dataset in computer vision. It consists of images of handwritten digits (0 - 9) and the correct digit classification. In this problem you will implement a deep network using PyTorch to classify MNIST digits. Specifically, you will explore what it really means for a network to be "deep", and how depth vs. width impacts the classification accuracy of a model. You will be allowed at most $N$ hidden units, and will be expected to design and implement a deep network that meets some performance baseline on the MNIST dataset.

\medskip

\problem \textbf{Installation} \textbf{[2 Points]}


Before any modeling can begin, PyTorch must be installed. PyTorch is an automatic differentiation framework that is widely used in machine learning research.  We will also need the \textbf{torchvision} package, which will make downloading the MNIST dataset much easier. 

If you use Google Colab (recommended), you won't need to install anything.

If you want to run PyTorch locally, follow the steps on \\
\url{https://pytorch.org/get-started/locally/#start-locally}. Select the 'Stable' build and your system information. We highly recommend using Python 3.6+. CUDA is not required for this class, but it is necessary if you want to do GPU-accelerated deep learning in the future.

Write down the version numbers for both \textbf{torch} and \textbf{torchvision} that you have installed. On Google Colab, you can find version numbers by running: \begin{verbatim}!pip list | grep torch\end{verbatim}

\begin{solution}
 The following versions were downloaded:
 torch                              1.10.2   
 torchvision                        0.11.3 
\end{solution}


\problem \textbf{The Data} \textbf{[5 Points]}

Load the MNIST dataset using torchvision; see the problem 2 sample code for how.

Image inputs in PyTorch are generally 3D tensors with the shape (no. of channels, height, width). Examine the input data. What are the height and width of the images? What do the values in each array index represent?  How many images are in the training set? How many are in the testing set? You can use the \textbf{imshow} function in matplotlib if you'd like to see the actual pictures (see the sample code).

\begin{subsolution}
 The images are 28 x 28. The values in each array range from 0 to 1 and represent the pixel value (brightness). We have 60000 images in the training set and and 10000 in the testing set.
\end{subsolution}

 \problem \textbf{Modeling Part 1} \textbf{[10 Points]}

 Using PyTorch's "Sequential" model class, build a deep network to classify the handwritten digits. You may \textbf{only} use the following layers:

 \begin{itemize}
  \item \textbf{Flatten:} Flattens any tensor into a single vector
  \item \textbf{Linear:} A fully-connected layer
  \item \textbf{ReLU (activation):} Sets negative inputs to 0
  \item \textbf{Softmax (activation):} Rescales input so that it can be interpreted as a (discrete) probability distribution.
  \item \textbf{Dropout:} Takes some probability and at every iteration sets weights to zero at random with that probability (effectively regularization)
\end{itemize}

A sample network with 20 hidden units is in the sample code file. (Note: You may use multiple layers as long as the total number of hidden units are within the limit. Activations, Dropout, and your last Linear layer do not count toward your hidden unit count, because the final layer is ``observed'' and not \emph{hidden}.)

Use categorical cross entropy as your loss function. There are also a number of optimizers  you can use (an optimizer is just a fancier version of SGD), and feel free to play around with them, but RMSprop and Adam are the most popular and will probably work best. You also should find the batch size and number of epochs that give you the best results (default is batch size = 32, epochs=10).

Look at the sample code to see how to train your model. You can tinker with the network architecture by swapping around layers and parameters.

\textbf{Your task}. Using at most 100 hidden units, build a network using only the allowed layers that achieves test accuracy of at least 0.975. Turn in the code of your model as well as the best test accuracy that it achieved.



\begin{solution}
 Link: \url{https://colab.research.google.com/drive/13PwQl0IiCOkUO7fQmlMWK41lryTzhP5L?usp=sharing}
 Final accuracy: 97.77\%
\end{solution}


 \problem \textbf{Modeling Part 2} \textbf{[8 Points]}

 Repeat problem C, except that now you may use 200 hidden units and must build a model with at least 2 hidden layers that achieves test accuracy of at least 0.98.

\begin{solution}
  Link: \url{https://colab.research.google.com/drive/1mYc1Ygq6XSXtnY1dVsyOnAOgjE9jfqoK?usp=sharing}
  Final accuracy: 98.09\%
\end{solution}

 \problem \textbf{Modeling Part 3} \textbf{[6 EC Points]}

 Repeat problem C, except that now you may use 1000 hidden units and must build a model with at least 3 hidden layers that achieves test accuracy of at least 0.983.

\begin{solution}
  Link: \url{https://colab.research.google.com/drive/1Z4H6-bZyVa-9qG6QVCYTkZkb-Wcl-r6E?usp=sharing}
  Final accuracy: 98.45\%
\end{solution}

 \newpage
 % problem 3
 \section{Convolutional Neural Networks  [40 Points]}
 \textit{Relevant Materials: Lecture on CNNs}

 \problem Zero Padding \textbf{[5 Points]}

 Consider a convolutional network in which we perform a convolution over each $8 \times 8$ patch of a $20 \times 20$ input image. It is common to zero-pad input images to allow for convolutions past the edges of the images. An example of zero-padding is shown below:

\begin{center}
  \includegraphics[width=.49\textwidth]{Plots/ConvolutionExample.png}
\end{center}
\begin{small}
Figure: A convolution being applied to a $2 \times 2$ patch (the red square) of a $3 \times 3$ image that has been zero-padded to allow convolutions past the edges of the image.
\end{small}

What is one benefit and one drawback to this zero-padding scheme (in contrast to an approach in which we only perform convolutions over patches entirely contained within an image)?

\begin{solution}
 One benefit of a zero-padding scheme is the preservation of the original size of the image, allowing us to extract the low-level features. However, equally, we will be wasting computational resources whenever we process these padded elements. 
\end{solution}

\subsection{5 x 5 Convolutions}

Consider a single convolutional layer, where your input is a $32 \times 32$ pixel, RGB image. In other words, the input is a $32 \times 32 \times 3$ tensor. Your convolution has:

\begin{itemize}
\item Size: $5 \times 5 \times 3$
\item Filters: 8
\item Stride (i.e. how much the filter is displaced after each application): 1
\item No zero-padding
\end{itemize}

\problem[2] What is the number of parameters (weights) in this layer, including a bias term for each filter?

\begin{subsolution}
 We have $(5\times 5\times3 +1 (\text{bias}))\times 8=608$ parameters for each filter.
\end{subsolution}

\problem[3] What is the shape of the output tensor? Remember that convolution is performed over the first two dimensions of the input only, and that a filter is applied to all channels.

\begin{subsolution}
 We have $(32-5+1)^2\times 3 \times 8 \text{filters} \rightarrow 28\times28\times3 \times 8$ filters.
\end{subsolution}

 \subsection{Max/Average Pooling}

Pooling is a downsampling technique for reducing the dimensionality of a layer's output. Pooling iterates across patches of an image similarly to a convolution, but pooling and convolutional layers compute their outputs differently: given a pooling layer $B$ with preceding layer $A$, the output of $B$ is some function (such as the max or average functions) applied to patches of $A$'s output.

Below is an example of max-pooling on a 2-D input space with a $2\times 2$ filter (the max function is applied to $2\times 2$ patches of the input) and a stride of 2 (so that the sampled patches do not overlap):

\begin{center}
  \includegraphics[width=.49\textwidth]{Plots/MaxPool.png}
\end{center}

Average pooling is similar except that you would take the average of each patch as its output instead of the maximum.

Consider the following 4 matrices:
$$
\begin{bmatrix}
    1 & 1 & 1 & 0 \\
    1 & 1 & 1 & 0 \\
    1 & 1 & 1 & 0 \\
    0 & 0 & 0 & 0
\end{bmatrix},
%
\begin{bmatrix}
    0 & 1 & 1 & 1 \\
    0 & 1 & 1 & 1 \\
    0 & 1 & 1 & 1 \\
    0 & 0 & 0 & 0
\end{bmatrix},
%
\begin{bmatrix}
	0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 1 \\
    0 & 1 & 1 & 1 \\
    0 & 1 & 1 & 1
\end{bmatrix},
%
\begin{bmatrix}
    0 & 0 & 0 & 0 \\
    1 & 1 & 1 & 0 \\
    1 & 1 & 1 & 0 \\
    1 & 1 & 1 & 0
\end{bmatrix}
$$
\problem[3]

Apply $2 \times 2$ average pooling with a stride of 2 to each of the above images.

\begin{subsolution}
  $$
  \begin{bmatrix}
    1 & 0.5 \\
    0.5 & 0.25 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    0.5 & 1 \\
    0.25 & 0.5 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    0.25 & 0.5 \\
    0.5 & 1 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    0.5 & 0.25 \\
    1 & 0.5 \\
  \end{bmatrix}
  %
  $$
\end{subsolution}

\problem[3]

Apply $2 \times 2$ max pooling with a stride of 2 to each of the above images.

\begin{subsolution}
  $$
  \begin{bmatrix}
    1 & 1 \\
    1 & 1 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    1 & 1 \\
    1 & 1 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    1 & 1 \\
    1 & 1 \\
  \end{bmatrix},
  %
  \begin{bmatrix}
    1 & 1 \\
    1 & 1 \\
  \end{bmatrix}
  %
  $$
\end{subsolution}

\problem[4]

Consider a scenario in which we wish to classify a dataset of images of various animals, where an animal may appear at various angles/locations of the image, and the image contains small amounts of noise (e.g. some pixels may be missing). Why might pooling be advantageous given these properties of our dataset?

\begin{subsolution}
 In pooling, our learning is relatively insensitive to individual pixels, meaning that things such as noise, different locations, etc., should not have such a significant impact on the learning, as long as most pixels have similar max and average values.
\end{subsolution}

\subsection{PyTorch implementation}
\problem[20]

Using PyTorch ``Sequential'' model class as you did in 2C, build a deep \emph{convolutional}
network to classify the handwritten digits in MNIST. You are now allowed to use
the following layers (but \textbf{only} the following):

 \begin{itemize}
  \item \textbf{Linear:} A fully-connected layer
  \begin{itemize}
    \item In convolutional networks, Linear (also called dense) layers are typically used to knit together higher-level feature representations.
    \item Particularly useful to map the 2D features resulting from the last convolutional layer to categories for classification (like the 1000 categories of ImageNet or the 10 categories of MNIST).
    \item Inefficient use of parameters and often overkill: for $A$ input activations and $B$ output activations, number of parameters needed scales as $O(AB)$.
  \end{itemize}
  \item \textbf{Conv2d:} A 2-dimensional convolutional layer
  \begin{itemize}
    \item The bread and butter of convolutional networks, conv layers impose a translational-invariance prior on a fully-connected network. By sliding filters across the image to form another image, conv layers perform ``coarse-graining'' of the image.
    \item Networking several convolutional layers in succession helps the convolutional network knit together more abstract representations of the input. As you go higher in a convolutional network, activations represent pixels, then edges, colors, and finally objects.
    \item More efficient use of parameters. For $N$ filters of $K \times K$ size on an input of size $L \times L$, the number of parameters needed scales as $O(N K^2)$. When $N, K$ are small, this can often beat the $O(L^4)$ scaling of a Linear layer applied to the $L^2$ pixels in the image.

  \end{itemize}
  \item \textbf{MaxPool2d:} A 2-dimensional max-pooling layer
  \begin{itemize}
    \item Another way of performing ``coarse-graining'' of images, max-pool layers are another way of ignoring finer-grained details by only considering maximum activations over small patches of the input.
    \item Drastically reduces the input size. Useful for reducing the number of parameters in your model.
    \item Typically used immediately following a series of convolutional-activation layers.
  \end{itemize}
  \item \textbf{BatchNorm2d:} Performs batch normalization (Ioffe and Szegedy, 2014). Normalizes the activations of previous layer to standard normal (mean 0, standard deviation 1).
  \begin{itemize}
    \item Accelerates convergence and improves performance of model, especially when saturating nonlinearities (sigmoid) are used.
    \item Makes model less sensitive to higher learning rates and initialization, and also acts as a form of regularization.
    \item Typically used immediately before nonlinearity (Activation) layers.
  \end{itemize}
  \item \textbf{Dropout:} Takes some probability and at every iteration sets weights to zero at random with that probability
  \begin{itemize}
    \item An effective form of regularization. During training, randomly selecting activations to shut off forces network to build in redundancies in the feature representation, so it does not rely on any single activation to perform classification.
  \end{itemize}
  \item \textbf{ReLU (activation):} Sets negative inputs to 0
  \item \textbf{Softmax (activation):} Rescales input so that it can be interpreted as a (discrete) probability distribution.
  \item \textbf{Flatten:} Flattens any tensor into a single vector (required in order to pass a 2D tensor output from a convolutional layer as input into Linear layers)
\end{itemize}

\textbf{Your tasks.} Build a network with only the allowed layers that achieves \textbf{test accuracy of at least 0.985}. You are required to use categorical cross entropy as your loss function and to train for 10 epochs with a batch size of 32. Note: your model must have fewer than 1 million parameters, as measured by the method given in the sample code.
Everything else can change: optimizer (e.g., RMSProp, Adam), initial learning rates, dropout probabilities, layerwise regularizer strengths, etc. You are not required to use all of the layers, but \emph{you must have
at least one dropout layer and one batch normalization layer in your final model}. Try to figure out the best possible architecture and hyperparameters given these building blocks!

In order to design your model, you should train your model for 1 epoch (batch size 32) and look at the final \textbf{test accuracy}  after training. This should take no more than 10 minutes, and should give you an immediate sense for how fast your network converges and how good it is.

Set the probabilities of your
dropout layers to 10 equally-spaced values $p \in [0, 1]$, train for 1 epoch, and report the final model accuracies for each.

You can perform all of your hyperparameter validation in this way: vary your parameters and train for an epoch. After you're satisfied with the model design, you should train your model for the full 10 epochs.

\textbf{In your submission.} Turn in the code of your model, the test accuracy for the 10 dropout probabilities $p \in [0, 1]$, and the
final test accuracy when your model
is trained for 10 epochs. We should have everything needed to reproduce your results.

Discuss what you found to be the most effective strategies in designing a convolutional network. Which regularization method was most effective (dropout, layerwise regularization, batch norm)?

Do you foresee any problem with this way of validating our hyperparameters? If so, why?

\emph{Hints:}

\begin{itemize}
	\item You are provided with a sample network that achieves a high accuracy. Starting with this network, modify some of the regularization parameters (layerwise regularization strength, dropout probabilities) to see if you can maximize the test accuracy. You can also add layers or modify layers (e.g.
	changing the convolutional kernel sizes, number of filters, stride, dilation, etc.) so long as the total number of parameters remains under the cap of 1 million.
  \item You may want to read up on successful convolutional architectures, and emulate some of their design principles. Please cite any idea you use that is not your own.
  \item To better understand the function of each layer, check the PyTorch documentation.
  \item Linear layers take in single vector inputs (ex: \emph{(784, )}) but Conv2D layers
	take in tensor inputs (ex: \emph{(28, 28, 1)}): width, height, and channels. Using the transformation \verb$transforms.ToTensor()$ when loading the dataset will reshape the training/test $X$ to a 4-dimensional tensor (ex: \emph{(num_examples, width, height, channels)}) and normalize values. For the MNIST dataset, \emph{channels=1}. Typical color images have 3 color channels, 1 for each color in RGB.
	\item If your model is running slowly on your CPU, try making each layer smaller
	and stacking more layers so you can leverage deeper representations.
	\item Other useful CNN design principles:
  \begin{itemize}
    \item CNNs perform well with many stacked convolutional layers, which develop increasingly large-scale representations of the input image.
    \item Dropout ensures that the learned representations are robust to some amount of noise.
    \item Batch norm is done after a convolutional or dense layer and immediately prior to an activation/nonlinearity layer.
    \item Max-pooling is typically done after a series of convolutions, in order to gradually reduce the size of the representation.
    \item Finally, the learned representation is passed into a dense layer (or two), and then filtered down to the final softmax layer.
  \end{itemize}
\end{itemize}
\begin{solution}
  Link: \url{https://colab.research.google.com/drive/1NIoYsNyDh3H1G0Pda8d_nn26Ayfap4Ch?usp=sharing}
  The following test accuracies were obtained for different dropouts:

    \begin{center}
    \begin{tabular}{|c|c|}\hline
    Dropout & Test Accuracy \\\hline
    0.1     & 0.9833        \\
    0.2     & 0.9765        \\
    0.3     & 0.9692        \\
    0.4     & 0.965         \\
    0.5     & 0.9514        \\
    0.6     & 0.9374        \\
    0.7     & 0.9117        \\
    0.8     & 0.8302        \\
    0.9     & 0.5715        \\
    1       & 0.098        \\\hline
    \end{tabular}
  \end{center}
  Based on the above, the optimal dropout is 0.1; using this gave the following values:
  \begin{itemize}
    \item Validation loss: 0.0502
    \item Validation accuracy: 0.9852
  \end{itemize}
  Values beyond 0.1 only lead to a deterioration in the model performance. Starting from the architecture provided and trying various approaches, it was found that, following the general CNN design principles given in the problem, adding a batch norm after each convolutional layer and prior to activation layers was the most effective regularisation method (in comparison to layerwise regularisation). This is primarily due to the simplicity of the dataset being trained on.

  The issue with this method of validating the hyperparameter is that we are using the testing data set to validate the hyperparameter. This could potentially lead to us overfitting the validation data. Without an additional testing set, it is difficult to evaluate the extent of this potential overfitting. 

  Furthermore, relying on just the first epoch to evaluate the performance of our model isn't very representative of the true performance. Due to the nature of SGD, it could be that the global optimum isn't reached until many epoches, in the case of all dropouts.
\end{solution}


\end{document}
